{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**We validate the deployed model using:**\n",
        "\n",
        "ISIC 2018 validation images - Ensures accuracy on standard clinical data\n",
        "External images (Google/web sources) - Tests real-world robustness across different cameras, lighting, and skin types\n",
        "\n",
        "Expected Results\n",
        "\n",
        "Processing time: ~2-5 seconds per image\n",
        "Output: Binary mask highlighting lesion boundaries\n",
        "Visualization: Red overlay on original image for intuitive interpretation\n",
        "\n",
        "\n",
        "Notebook Structure:\n",
        "\n",
        "Setup & Installation\n",
        "Model Architecture Definition\n",
        "Model Loading & Initialization\n",
        "Inference Function\n",
        "Gradio Interface Deployment\n",
        "\n",
        "Let's get started! ðŸ‘‡"
      ],
      "metadata": {
        "id": "DGrWOE9XTDky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vIvOAWT0LM5b"
      },
      "outputs": [],
      "source": [
        "# Install required packages (run this cell first)\n",
        "!pip install torch torchvision albumentations pillow gradio -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch33KL2ALZrN",
        "outputId": "8dc69c6b-1b9d-4628-9ae8-2612fb6bbe20"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet50, resnet101\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHqfmcUuLjaH",
        "outputId": "2b24874d-2050-43cc-ff13-273506482a11"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ MODEL ARCHITECTURE ============\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=256):\n",
        "        super(ASPP, self).__init__()\n",
        "        atrous_rates = [6, 12, 18]\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv2 = self._make_atrous_conv(in_channels, out_channels, atrous_rates[0])\n",
        "        self.conv3 = self._make_atrous_conv(in_channels, out_channels, atrous_rates[1])\n",
        "        self.conv4 = self._make_atrous_conv(in_channels, out_channels, atrous_rates[2])\n",
        "\n",
        "        self.global_avg_pool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv_fusion = nn.Sequential(\n",
        "            nn.Conv2d(out_channels * 5, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "    def _make_atrous_conv(self, in_channels, out_channels, dilation):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=dilation,\n",
        "                      dilation=dilation, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.shape[2:]\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(x)\n",
        "        x3 = self.conv3(x)\n",
        "        x4 = self.conv4(x)\n",
        "        x5 = self.global_avg_pool(x)\n",
        "        x5 = F.interpolate(x5, size=size, mode='bilinear', align_corners=True)\n",
        "        x = torch.cat([x1, x2, x3, x4, x5], dim=1)\n",
        "        return self.conv_fusion(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, low_level_channels, num_classes):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.conv_low_level = nn.Sequential(\n",
        "            nn.Conv2d(low_level_channels, 48, 1, bias=False),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv_decoder = nn.Sequential(\n",
        "            nn.Conv2d(304, 304, 3, padding=1, groups=304, bias=False),\n",
        "            nn.BatchNorm2d(304),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(304, 256, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Conv2d(256, 256, 3, padding=1, groups=256, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Conv2d(256, num_classes, 1)\n",
        "\n",
        "    def forward(self, x, low_level_feat):\n",
        "        low_level_feat = self.conv_low_level(low_level_feat)\n",
        "        x = F.interpolate(x, size=low_level_feat.shape[2:],\n",
        "                          mode='bilinear', align_corners=True)\n",
        "        x = torch.cat([x, low_level_feat], dim=1)\n",
        "        x = self.conv_decoder(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "class DeepLabV3Plus(nn.Module):\n",
        "    def __init__(self, num_classes=1, backbone='resnet50',\n",
        "                 pretrained=True, output_stride=16):\n",
        "        super(DeepLabV3Plus, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.output_stride = output_stride\n",
        "\n",
        "        if backbone == 'resnet50':\n",
        "            resnet = resnet50(pretrained=pretrained)\n",
        "        elif backbone == 'resnet101':\n",
        "            resnet = resnet101(pretrained=pretrained)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
        "\n",
        "        self.conv1 = resnet.conv1\n",
        "        self.bn1 = resnet.bn1\n",
        "        self.relu = resnet.relu\n",
        "        self.maxpool = resnet.maxpool\n",
        "\n",
        "        self.layer1 = resnet.layer1\n",
        "        self.layer2 = resnet.layer2\n",
        "        self.layer3 = resnet.layer3\n",
        "        self.layer4 = resnet.layer4\n",
        "\n",
        "        if output_stride == 16:\n",
        "            self._modify_resnet_layer(self.layer4, dilation=2)\n",
        "        elif output_stride == 8:\n",
        "            self._modify_resnet_layer(self.layer3, dilation=2)\n",
        "            self._modify_resnet_layer(self.layer4, dilation=4)\n",
        "\n",
        "        self.aspp = ASPP(in_channels=2048, out_channels=256)\n",
        "        self.decoder = Decoder(low_level_channels=256, num_classes=num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _modify_resnet_layer(self, layer, dilation):\n",
        "        for block in layer:\n",
        "            block.conv2.dilation = (dilation, dilation)\n",
        "            block.conv2.padding = (dilation, dilation)\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.decoder.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_size = x.shape[2:]\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        low_level_feat = self.layer1(x)\n",
        "\n",
        "        x = self.layer2(low_level_feat)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.aspp(x)\n",
        "        x = self.decoder(x, low_level_feat)\n",
        "\n",
        "        x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=True)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============ LOAD MODEL ============\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Loading Model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define model path - UPDATE THIS PATH!\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Project/best_model.pth\"\n",
        "\n",
        "# Check if file exists\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"\\nâŒ ERROR: Model file not found at: {MODEL_PATH}\")\n",
        "    print(\"\\nChecking your Drive structure...\")\n",
        "\n",
        "    # Try to find the file\n",
        "    drive_root = \"/content/drive/MyDrive\"\n",
        "    if os.path.exists(drive_root):\n",
        "        print(f\"\\nContents of MyDrive:\")\n",
        "        for item in os.listdir(drive_root)[:20]:\n",
        "            print(f\"  - {item}\")\n",
        "\n",
        "    raise FileNotFoundError(f\"Please update MODEL_PATH to the correct location of best_model.pth\")\n",
        "\n",
        "print(f\"âœ“ Found model at: {MODEL_PATH}\")\n",
        "\n",
        "# Initialize model\n",
        "model = DeepLabV3Plus(\n",
        "    num_classes=1,\n",
        "    backbone='resnet50',\n",
        "    pretrained=False,\n",
        "    output_stride=16\n",
        ").to(device)\n",
        "\n",
        "# Load weights\n",
        "checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
        "if \"model_state_dict\" in checkpoint:\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "else:\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "model.eval()\n",
        "print(\"âœ“ Model loaded successfully!\\n\")\n",
        "\n",
        "\n",
        "# ============ PREPROCESSING ============\n",
        "transform = A.Compose([\n",
        "    A.Resize(height=512, width=512),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BwEQh-1LxhO",
        "outputId": "f951adea-c6e5-4359-e1e4-4e03f10525b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Loading Model...\n",
            "============================================================\n",
            "âœ“ Found model at: /content/drive/MyDrive/Project/best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Model loaded successfully!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Deployment**\n",
        "Overview\n",
        "Deploying our trained DeepLabV3+ model as an interactive web application using Gradio. Users can upload dermoscopic images and get instant lesion segmentation results.\n",
        "Testing Strategy\n",
        "We test the deployed model with images from two sources:\n",
        "1. ISIC 2018 Dataset - Validation images (unseen during training) to verify accuracy on standard dermoscopic imaging\n",
        "2. External Images - Real-world images from Google/medical websites to test robustness on varied cameras, lighting, and skin types\n",
        "This ensures our model works both on clinical-grade data and real-world scenarios.\n",
        "What You'll Get\n",
        "\n",
        "Original Image - Your uploaded image\n",
        "Binary Mask - White = lesion, Black = background\n",
        "Overlay - Lesion highlighted in red\n",
        "\n",
        "Run the cell below to launch the interface!"
      ],
      "metadata": {
        "id": "Lk1mIXU5SiSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ INFERENCE FUNCTION ============\n",
        "def predict_lesion(image):\n",
        "    \"\"\"\n",
        "    Segments skin lesion from input image\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "\n",
        "    Returns:\n",
        "        original, mask, overlay images\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Ensure RGB\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        original = image.copy()\n",
        "        orig_size = original.size\n",
        "\n",
        "        # Preprocess\n",
        "        img_array = np.array(original)\n",
        "        transformed = transform(image=img_array)\n",
        "        img_tensor = transformed[\"image\"].unsqueeze(0).to(device)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            output = model(img_tensor)\n",
        "            prob = torch.sigmoid(output)[0, 0].cpu().numpy()\n",
        "\n",
        "        # Create mask\n",
        "        binary_mask = (prob > 0.5).astype(np.uint8) * 255\n",
        "        mask_pil = Image.fromarray(binary_mask, mode='L')\n",
        "        mask_pil = mask_pil.resize(orig_size, Image.NEAREST)\n",
        "\n",
        "        # Create overlay\n",
        "        orig_array = np.array(original)\n",
        "        mask_array = np.array(mask_pil)\n",
        "\n",
        "        overlay_array = orig_array.copy()\n",
        "        lesion_pixels = mask_array > 127\n",
        "\n",
        "        # Blend red color with original\n",
        "        overlay_array[lesion_pixels] = (\n",
        "            orig_array[lesion_pixels] * 0.4 +\n",
        "            np.array([255, 0, 0]) * 0.6\n",
        "        ).astype(np.uint8)\n",
        "\n",
        "        overlay_pil = Image.fromarray(overlay_array)\n",
        "\n",
        "        return original, mask_pil, overlay_pil\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# ============ GRADIO INTERFACE ============\n",
        "print(\"=\"*60)\n",
        "print(\"Creating Gradio Interface...\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=predict_lesion,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload Dermoscopic Image\"),\n",
        "    outputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Original Image\"),\n",
        "        gr.Image(type=\"pil\", label=\"Segmentation Mask (Binary)\"),\n",
        "        gr.Image(type=\"pil\", label=\"Overlay (Red = Lesion)\")\n",
        "    ],\n",
        "    title=\"ðŸ”¬ Skin Lesion Segmentation - DeepLabV3+\",\n",
        "    description=\"\"\"\n",
        "    ### ISIC 2018 Challenge - Automated Skin Lesion Boundary Detection\n",
        "\n",
        "    **How to use:**\n",
        "    1. Upload a dermoscopic image of a skin lesion\n",
        "    2. The model will automatically segment the lesion boundary\n",
        "    3. View results: Original, Binary Mask, and Highlighted Overlay\n",
        "\n",
        "    **Model:** DeepLabV3+ with ResNet-50 backbone\n",
        "    \"\"\",\n",
        "    allow_flagging=\"never\",\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "# Launch interface\n",
        "print(\"ðŸš€ Launching Gradio UI...\\n\")\n",
        "interface.launch(\n",
        "    share=True,      # Creates public URL (lasts 72 hours)\n",
        "    debug=True       # Shows detailed errors\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "scKH5974L-pG",
        "outputId": "aa5ad92b-e311-49a6-ef39-e1c9c4a25b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Creating Gradio Interface...\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Launching Gradio UI...\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a45728380bf6444a59.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a45728380bf6444a59.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2975507468.py:32: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_pil = Image.fromarray(binary_mask, mode='L')\n",
            "/tmp/ipython-input-2975507468.py:32: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_pil = Image.fromarray(binary_mask, mode='L')\n",
            "/tmp/ipython-input-2975507468.py:32: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_pil = Image.fromarray(binary_mask, mode='L')\n",
            "/tmp/ipython-input-2975507468.py:32: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_pil = Image.fromarray(binary_mask, mode='L')\n",
            "/tmp/ipython-input-2975507468.py:32: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_pil = Image.fromarray(binary_mask, mode='L')\n",
            "/tmp/ipython-input-2975507468.py:32: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_pil = Image.fromarray(binary_mask, mode='L')\n",
            "/tmp/ipython-input-2975507468.py:32: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_pil = Image.fromarray(binary_mask, mode='L')\n",
            "/tmp/ipython-input-2975507468.py:32: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_pil = Image.fromarray(binary_mask, mode='L')\n",
            "/tmp/ipython-input-2975507468.py:32: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_pil = Image.fromarray(binary_mask, mode='L')\n",
            "/tmp/ipython-input-2975507468.py:32: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_pil = Image.fromarray(binary_mask, mode='L')\n",
            "/tmp/ipython-input-2975507468.py:32: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_pil = Image.fromarray(binary_mask, mode='L')\n"
          ]
        }
      ]
    }
  ]
}